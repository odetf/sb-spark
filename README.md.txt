Данный репозиторий содержит решения домашек по курсу от Newprolab ["Apache Spark и Scala для дата-инжиниринга"](https://newprolab.com/ru/spark-de)

На данный момент в репозитории 7 решенных домашних заданий:

1. lab01
 Требовалось с помощью Scala вычитать файл с данными по просмотрам фильмов из текстовго файла u.data. На основе данных требовалось создать json-файл, состоящий из двух полей формата массив. В первом поле (hist_film) требовалось рассчитать, сколько раз встречается каждая оценка (диапазон оценок от 1 до 5), Во втором поле (hist_all) - сделать то же самое для всех фильмов датасета

2. lab02
 Требовалось вычитать логи посещения пользователей в интернете. Нужно было составить список из топ-100 доменов, наиболее релевантных для категории пользователей "Автомобилисты". Каждый файл логов состоял из кортежей формата: (UUID пользователя, timestamp со временем добавления записи в форме UNIX, экранированный строковый URL). Разделитель строк - \t
 Дополнительно был файл со списком UUID-ов автолюбителей.
 Релевантность определялась следующим образом: в числителе стояла вероятность того, что наугад выбранная строка лога будет одновременно относиться к пользователю “Автомобилист” и интересующему домену, в квадрате; в знаменателе стояло произведение вероятности того, что наугад выбранная строка будет относиться к интересующему домену, и вероятности того, что наугад выбранная строка будет относиться к пользователю “Автомобилист”
Полученный результат требовалось сохранить в текстовый файл

3. lab03
 Требовалось на основе разных источников данных создать витрину (в формате широкой денормализованной таблицы) и записать её в Postgres. Используемые источники данных:
    - таблица с информацией о возрасте и поле клиента в Cassandra
    - логи посещения интернет-магазина в ElasticSearch
    - логи посещения веб-сайта в формате json
    - справочник категорий веб-сайтов в Postgres

    Финальная витрина должна была включать следующие поля: UUID пользователя, его пол, категорию возраста (18-24, 25-34, 35-44, 45-54, >=55), категории товаров из интернет-магазин (поле формата shop_catN, где N - наименование категории в формате under_score), категории веб-сайтов (поле формата wab_catN, где N - наименование категории в формате under_score)
    
4. lab04a
Лабораторная работы была направлена на обработку данных из Kafka (пакетная поставка). Требовалось написать класс filter, который принимал бы 3 параметра: 
    - топик Кафка (spark.filter.topic_name), 
    - офсет топика (spark.filter.offset), 
    - директория для результирующих файлов (spark.filter.output_dir_prefix)

   Данный класс должен считывать данные из указанного в параметрах топика Кафки с данными о визитах пользователя на странички товаров (клики) и о покупках. Нужно вычитать данные, записать их в результирующей директории в две разные директории (view или buy, в зависимости от того, это визит или покупка) и распартиционировать данные по дате в формате "YYYYMMDD".
   
5. lab04b
Лабораторная работа была направлена на чтение потоковых данных из Kafka с размером микробатча равным 5 сек и временным периодом с интервалом 1 час. Нужно считать данные из топика, рассчитать следующие агрегированные метрики:
    - общую сумму продаж за продаж (revenue)
    - число идентифицированных посещений (visitors) за период
    - число покупок (purchases) за период
    - средний чек (aov) за период

   Полученные данные нужно записать в другой топик.
   
6. lab05
Целью лабораторной работы было создать на основе записанных в lab04a данных матрицу users x items. У этой матрицы строки - uuid-ы пользователей, столбцы - товары, которые просмотрел данный юзер или купил. Для каждого просмотренного товара должна быть заведена колонка "view_item_id", а для каждого купленного товара – колонка "buy_item_id". Для каждого юзера в каждой ячейке должно быть указано сколько раз он или она посетили или купили данный товар (0 если ни разу). Названия полей item_id должны быть нормализованы – приведены к нижнему регистру, пробелы и тире заменены на подчерк. Spark-приложение при запуске должно было читать следующие параметры:
    - директория с входными файлами (spark.users_items.input_dir)
    - директория с результирующими файлами (spark.users_items.output_dir)
    - флаг обновления матрицы (spark.users_items.update=1)

   Если флаг spark.users_items.update равен 0, то нужно создать новую матрицу users x items согласно данным из spark.users_items.input_dir в директории spark.users_items.output_dir. Если же spark.users_items.update равен 1, необходимо считать из директории spark.users_items.output_dir последнее значение матрицы, добавить данные из input_dir и записать новую матрицу по новому пути в соответствии с датой последних данных из input_dir.

7. lab06
Цель данной лабораторной: к матрице из lab05 добавить данные о посещениях веб-сайтов пользователями, которые использовались в lab03. Для этого нужно аналогично lab03 извлечь из url домены. Для каждого юзера в колонке domain_features должен содержаться вектор с числами (количеством) посещений каждого сайта из топ-1000 по посещениям или 0, если не посещал. Из временных меток необходимо рассчитать следующие колонки:
    - число посещений по дням недели
    - число посещений по часам в сутках
    - долю посещений в рабочие часы "web_fraction_work_hours" и долю посещений в вечерние часы "web_fraction_evening_hours"

   Затем необходимо объединить этот датасет с матрицей users x items и сохранить полученный датасет в формате parquet.